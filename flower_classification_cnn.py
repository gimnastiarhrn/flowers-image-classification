# -*- coding: utf-8 -*-
"""Klasifikasi_Bunga_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t76O2xZHK0n2_S7iKHysgjTaVcA0H23D

# Proyek Klasifikasi Gambar: Flowers Dataset
- **Nama:** Gymnastiar Harun
- **Email:** gimnnastiarhrn@gmail.com
- **ID Dicoding:** gimnastiarhrn

## Import Semua Packages/Library yang Digunakan
"""

import os
import pathlib
import shutil
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

"""## Data Preparation"""

#from google.colab import files
#files.upload()

import kagglehub

# Download latest version
path = kagglehub.dataset_download("gunavenkatdoddi/eye-diseases-classification")

print("Path to dataset files:", path)

dataset_dir = pathlib.Path('/kaggle/input/eye-diseases-classification/dataset')

"""### Data Loading"""

# Buat folder baru untuk dataset yang sudah dibagi
base_dir = 'flowers_split'
train_dir = os.path.join(base_dir, 'train')
val_dir = os.path.join(base_dir, 'val')
test_dir = os.path.join(base_dir, 'test')

# Fungsi untuk membuat folder jika belum ada
def make_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

make_dir(train_dir)
make_dir(val_dir)
make_dir(test_dir)

# Ambil semua kelas (nama folder)
class_names = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]

# Buat folder kelas di train, val, test
for cls in class_names:
    make_dir(os.path.join(train_dir, cls))
    make_dir(os.path.join(val_dir, cls))
    make_dir(os.path.join(test_dir, cls))

# Kumpulkan semua file gambar dan labelnya
file_paths = []
labels = []

for cls in class_names:
    cls_folder = os.path.join(dataset_dir, cls)
    files = os.listdir(cls_folder)
    for f in files:
        file_paths.append(os.path.join(cls_folder, f))
        labels.append(cls)

# Bagi dataset menjadi train dan temp
train_files, temp_files, train_labels, temp_labels = train_test_split(
    file_paths, labels, test_size=0.4, stratify=labels, random_state=42)

# Bagi temp menjadi validation dan test
val_files, test_files, val_labels, test_labels = train_test_split(
    temp_files, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)

# Fungsi untuk copy file ke folder tujuan
def copy_files(file_list, label_list, target_dir):
    for file_path, label in zip(file_list, label_list):
        dest_dir = os.path.join(target_dir, label)
        shutil.copy(file_path, dest_dir)

# Copy file ke folder train, val, test
copy_files(train_files, train_labels, train_dir)
copy_files(val_files, val_labels, val_dir)
copy_files(test_files, test_labels, test_dir)

print("Pembagian dataset selesai.")
print(f"Train set: {len(train_files)} gambar")
print(f"Validation set: {len(val_files)} gambar")
print(f"Test set: {len(test_files)} gambar")

"""### Data Preprocessing

#### Split Dataset
"""

# Tambahkan data augmentasi pada pipeline data training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='wrap'
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

IMG_SIZE = (224, 224)  # Ukuran gambar bisa disesuaikan
BATCH_SIZE = 32

train_ds = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True
)

val_ds = val_datagen.flow_from_directory(
    val_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

test_ds = test_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

class_names = list(train_ds.class_indices.keys())
num_classes = len(class_names)
print("Kelas:", class_names)


# Terapkan augmentasi pada train_ds
#train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))

# Optimasi pipeline data
#AUTOTUNE = tf.data.AUTOTUNE
#train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
#test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""## Modelling"""

model = models.Sequential([
    Input(shape=IMG_SIZE + (3,)),
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(256, (3,3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2,2),

    layers.Flatten(),
    #layers.Dropout(0.5),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(len(class_names), activation='softmax')
])

model.summary()

model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1)

epochs = 50

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs,
    callbacks=[early_stopping, reduce_lr]
)

"""## Evaluasi dan Visualisasi"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc))

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

test_loss, test_acc = model.evaluate(test_ds)
print(f"Akurasi pada test set: {test_acc*100:.2f}%")

# make predictions on test data
test_predictions = model.predict(test_ds)

# convert predictions from one-hot encoding to class labels
test_labels = np.argmax(test_predictions, axis=1)

# get true labels of test data
true_labels = test_ds.classes

# compute confusion matrix
cm = confusion_matrix(true_labels, test_labels)

# display confusion matrix
print('Confusion Matrix:')
print(cm)

sns.heatmap(cm ,annot=True, cmap= "Blues", fmt="d", square=True,
            xticklabels=class_names, yticklabels=class_names)

"""## Konversi Model"""

saved_model_dir = 'saved_model' # Changed to a directory name without extension
tf.saved_model.save(model, saved_model_dir) # Save the model as a SavedModel

# Konversi ke TF-Lite
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # Update the path here
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)


# Konversi ke TFJS
# Perlu install tensorflowjs terlebih dahulu di environment kamu

!pip install tensorflowjs
import subprocess
subprocess.run(['tensorflowjs_converter', '--input_format', 'tf_saved_model',
                '--output_format', 'tfjs_graph_model', saved_model_dir, 'tfjs_model'])

"""## Inference (Optional)"""

!pip install pipreqs
from google.colab import drive
drive.mount('/content/drive')
!pipreqs "/content/drive/MyDrive/Colab Notebooks" --scan-notebooks